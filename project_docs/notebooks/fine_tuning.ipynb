{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Hugging Face Models on Vertex AI for Dexter Platform\n",
    "\n",
    "**Project Goal:** Implement and deploy fine-tuned ML models on Hugging Face, deployable to Vertex AI, to support Dexter's core functionalities (narrative detection, actor analysis, media analysis, etc.).\n",
    "\n",
    "This notebook provides a template for fine-tuning various Hugging Face models and deploying them to Google Cloud Vertex AI. It covers:\n",
    "1. Setup and Prerequisites\n",
    "2. Configuration\n",
    "3. Data Handling (General Approach)\n",
    "4. Model Selection\n",
    "5. Fine-tuning Example (Text Classification for Narrative Detection)\n",
    "6. Model Evaluation\n",
    "7. Deployment to Vertex AI\n",
    "8. Making Predictions\n",
    "9. Adapting for Other Modalities/Tasks\n",
    "10. Next Steps and Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Install Required Libraries\n",
    "Ensure you have the necessary Python libraries installed. You can install them using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets torch torchvision torchaudio google-cloud-aiplatform scikit-learn pandas accelerate sentencepiece protobuf huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from google.cloud import aiplatform\n",
    "from huggingface_hub import HfFolder, notebook_login, whoami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Authentication\n",
    "\n",
    "**Hugging Face Hub:**\n",
    "- You'll need a Hugging Face account and an API token to download models/datasets and push your fine-tuned models.\n",
    "- You can login using `notebook_login()` or set the `HF_TOKEN` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell and follow the instructions to login to Hugging Face\n",
    "# notebook_login() \n",
    "\n",
    "# Alternatively, ensure HF_TOKEN is set as an environment variable if you are running non-interactively\n",
    "# For example, in your terminal: export HF_TOKEN='your_hf_token_here'\n",
    "\n",
    "# Verify login (optional)\n",
    "try:\n",
    "    hf_user = whoami()\n",
    "    print(f\"Successfully logged in to Hugging Face as: {hf_user['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Hugging Face login check failed. Ensure you are logged in or HF_TOKEN is set. Error: {e}\")\n",
    "    print(\"You might need to run: from huggingface_hub import notebook_login; notebook_login()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Google Cloud Platform (GCP) & Vertex AI:**\n",
    "- Ensure you have a GCP project with billing enabled. <mcreference link=\"https://codelabs.developers.google.com/vertex-training-autopkg\" index=\"3\">3</mcreference>\n",
    "- Enable the Vertex AI API in your GCP project.\n",
    "- Authenticate your environment. If running locally or on Colab, you can use: `gcloud auth application-default login`.\n",
    "- If running in a GCP environment (e.g., Vertex AI Workbench), authentication might be pre-configured.\n",
    "- You'll need a service account with appropriate permissions (e.g., Vertex AI User, Storage Object Admin if using GCS for data/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to authenticate with GCP if you haven't already:\n",
    "# From your terminal: gcloud auth application-default login\n",
    "# Or set GOOGLE_APPLICATION_CREDENTIALS environment variable to the path of your service account key file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "Set up your project-specific configurations here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP Project Configuration\n",
    "PROJECT_ID = \"your-gcp-project-id\"  # Replace with your GCP project ID\n",
    "REGION = \"us-central1\"  # Replace with your desired GCP region\n",
    "BUCKET_NAME = \"your-gcs-bucket-name\" # Replace with your GCS bucket name (e.g., for staging models)\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "# Hugging Face Configuration\n",
    "HF_USERNAME = hf_user['name'] if 'hf_user' in locals() and hf_user else \"your-hf-username\" # Your Hugging Face username/org\n",
    "\n",
    "# Model Configuration (Example for Text Classification)\n",
    "BASE_MODEL_ID = \"bert-base-multilingual-cased\" # Example model, choose based on your task and Vertex AI compatibility <mcreference link=\"https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hugging-face-models\" index=\"1\">1</mcreference>\n",
    "FINETUNED_MODEL_NAME = f\"{BASE_MODEL_ID.split('/')[-1]}-dexter-narrative\" # Name for your fine-tuned model\n",
    "HF_HUB_MODEL_ID = f\"{HF_USERNAME}/{FINETUNED_MODEL_NAME}\" # ID for pushing to Hugging Face Hub\n",
    "\n",
    "# Dataset Configuration (Example)\n",
    "# DATASET_PATH = \"path/to/your/dataset.csv\" # Or Hugging Face dataset ID like 'imdb'\n",
    "# TEXT_COLUMN = \"text_column_name\"\n",
    "# LABEL_COLUMN = \"label_column_name\"\n",
    "NUM_LABELS = 2 # Adjust based on your classification task (e.g., number of narrative types)\n",
    "\n",
    "# Training Configuration\n",
    "OUTPUT_DIR = f\"./results/{FINETUNED_MODEL_NAME}\"\n",
    "LOGGING_DIR = f\"./logs/{FINETUNED_MODEL_NAME}\"\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 16\n",
    "NUM_TRAIN_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Vertex AI Deployment Configuration\n",
    "VERTEX_MODEL_DISPLAY_NAME = FINETUNED_MODEL_NAME.replace(\"-\", \"_\") # Vertex AI display names use underscores\n",
    "VERTEX_ENDPOINT_DISPLAY_NAME = f\"{VERTEX_MODEL_DISPLAY_NAME}_endpoint\"\n",
    "# Choose a serving container. For many Hugging Face Transformers, PyTorch DLCs are suitable. <mcreference link=\"https://medium.com/google-cloud/open-models-on-vertex-ai-with-hugging-face-lets-get-started-b6bbc750e734\" index=\"2\">2</mcreference>\n",
    "# Find the latest appropriate container URI from: https://cloud.google.com/deep-learning-containers/docs/choosing-container\n",
    "# Example for PyTorch 2.2, CUDA 12.1, Python 3.11 (ensure compatibility with your model and transformers version)\n",
    "SERVING_CONTAINER_IMAGE_URI = \"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/pytorch-gpu.2-2.cu121.py310:latest\" # Check for latest or more specific HF inference containers\n",
    "# Or use a Hugging Face specific inference container if available and suitable, e.g.:\n",
    "# SERVING_CONTAINER_IMAGE_URI = \"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-pytorch-inference-cu121.2-2.transformers.4-44.ubuntu2204.py311\" <mcreference link=\"https://medium.com/google-cloud/open-models-on-vertex-ai-with-hugging-face-lets-get-started-b6bbc750e734\" index=\"2\">2</mcreference>\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
    "\n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"GCS Bucket URI: {BUCKET_URI}\")\n",
    "print(f\"Base Model ID: {BASE_MODEL_ID}\")\n",
    "print(f\"Fine-tuned Model Name (local): {FINETUNED_MODEL_NAME}\")\n",
    "print(f\"Hugging Face Hub Model ID: {HF_HUB_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Handling (General Approach)\n",
    "\n",
    "Data for fine-tuning can come from various sources as per Dexter's pipeline:\n",
    "- **Ingestion**: API connectors, web scrapers for real-time data.\n",
    "- **Storage**: Cloud-based relational (PostgreSQL) or NoSQL (MongoDB) databases, or GCS buckets.\n",
    "\n",
    "This notebook will demonstrate loading data from a local file (e.g., CSV) or directly from the Hugging Face `datasets` library. For production, you would integrate this with your data ingestion and processing pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Load and Prepare Dataset (Example: Text Classification)\n",
    "\n",
    "This section needs to be adapted based on your specific dataset and task. \n",
    "- For **Text Classification (Narrative Detection, Sentiment Analysis)**: You'll need a dataset with text samples and corresponding labels.\n",
    "- For **Image Classification (Memes, Stickers)**: You'll need a dataset of images and their labels.\n",
    "- For **Audio/Video**: Datasets with audio/video files and transcriptions or classifications.\n",
    "\n",
    "**Supported Languages for Dexter:** English, Pidgin, Hausa, Yoruba, Igbo. Ensure your dataset covers these or use multilingual models and data augmentation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load a dummy dataset for text classification\n",
    "# Replace this with your actual data loading logic\n",
    "\n",
    "# Option 1: Load from a Hugging Face dataset (e.g., 'imdb' for sentiment analysis)\n",
    "# raw_datasets = load_dataset('imdb') # Replace 'imdb' with a relevant multilingual dataset if possible\n",
    "\n",
    "# Option 2: Create a dummy dataset for demonstration (replace with your actual data)\n",
    "texts = [\n",
    "    \"This is a positive narrative about community development.\",\n",
    "    \"Negative sentiment detected in this online post.\",\n",
    "    \"Neutral statement regarding recent events.\",\n",
    "    \"Another piece of positive news from the region.\",\n",
    "    \"Concerning report about misinformation spread.\",\n",
    "    # Add more examples, especially in Nigerian languages/Pidgin if available\n",
    "    \"Dis na good tori about how pipo dey help demself.\", # Pidgin example (positive)\n",
    "    \"Wetin dis man dey talk sef? E be like say na lie.\" # Pidgin example (negative/skeptical)\n",
    "]\n",
    "labels = [1, 0, 2, 1, 0, 1, 0] # Example: 0 for Negative, 1 for Positive, 2 for Neutral (adjust NUM_LABELS accordingly)\n",
    "\n",
    "# Ensure NUM_LABELS matches the number of unique labels in your actual dataset\n",
    "if len(texts) > 0:\n",
    "    unique_labels = sorted(list(set(labels)))\n",
    "    NUM_LABELS = len(unique_labels)\n",
    "    print(f\"Number of unique labels found: {NUM_LABELS}\")\n",
    "    print(f\"Unique labels: {unique_labels}\")\n",
    "\n",
    "    # Create a pandas DataFrame (common for local data)\n",
    "    df = pd.DataFrame({'text': texts, 'label': labels})\n",
    "\n",
    "    # Split data into train and test/validation sets\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'] if NUM_LABELS > 1 and len(df['label'].unique()) > 1 else None)\n",
    "\n",
    "    # Convert pandas DataFrames to Hugging Face Dataset objects\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    # Create a DatasetDict (standard format for Hugging Face Trainer)\n",
    "    raw_datasets = DatasetDict({
",
    "        'train': train_dataset,
",
    "        'test': test_dataset
",
    "    })\n",
    "\n",
    "    print(\"Raw Datasets:\")\n",
    "    print(raw_datasets)\n",
    "else:\n",
    "    print(\"No data loaded. Please provide a dataset.\")\n",
    "    raw_datasets = None # Ensure raw_datasets is defined even if empty\n",
    "\n",
    "# If loading from a CSV file:\n",
    "# DATASET_PATH = 'your_dataset.csv'\n",
    "# TEXT_COLUMN = 'text'\n",
    "# LABEL_COLUMN = 'label'\n",
    "# \n",
    "# if os.path.exists(DATASET_PATH):\n",
    "#     df = pd.read_csv(DATASET_PATH)\n",
    "#     # Preprocess labels if they are not integers (e.g., string labels to integer IDs)\n",
    "#     if df[LABEL_COLUMN].dtype == 'object':\n",
    "#         unique_labels_list = df[LABEL_COLUMN].unique().tolist()\n",
    "#         label2id = {label: i for i, label in enumerate(unique_labels_list)}
",
    "#         id2label = {i: label for i, label in enumerate(unique_labels_list)}
",
    "#         NUM_LABELS = len(unique_labels_list)
",
    "#         df['label_id'] = df[LABEL_COLUMN].map(label2id)
",
    "#         print(f\"Mapped labels: {label2id}\")
",
    "#     else:
",
    "#         # Assume labels are already integers
",
    "#         label2id = None # Or create from unique values if needed
",
    "#         id2label = None
",
    "#         NUM_LABELS = df[LABEL_COLUMN].nunique()
",
    "#         df['label_id'] = df[LABEL_COLUMN]
",
    "#     \n",
    "#     train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label_id'] if NUM_LABELS > 1 else None)\n",
    "#     train_dataset = Dataset.from_pandas(train_df[[TEXT_COLUMN, 'label_id']].rename(columns={'label_id': 'label'}))\n",
    "#     test_dataset = Dataset.from_pandas(test_df[[TEXT_COLUMN, 'label_id']].rename(columns={'label_id': 'label'}))\n",
    "#     raw_datasets = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "#     print(raw_datasets)\n",
    "# else:\n",
    "#     print(f\"Dataset file not found at {DATASET_PATH}. Please provide a valid path.\")\n",
    "#     raw_datasets = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Tokenization\n",
    "Tokenize the text data to prepare it for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Assuming your text column is named 'text' in the Dataset object\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "if raw_datasets:\n",
    "    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "    # Remove original text column as it's no longer needed after tokenization and can cause issues with some models/Trainer\n",
    "    # And rename 'label_id' to 'labels' if you used 'label_id' earlier, Trainer expects 'labels'\n",
    "    columns_to_remove = [col for col in ['text', '__index_level_0__'] if col in tokenized_datasets['train'].column_names] # '__index_level_0__' is added by from_pandas\n",
    "    tokenized_datasets = tokenized_datasets.remove_columns(columns_to_remove)\n",
    "    # tokenized_datasets = tokenized_datasets.rename_column(\"label_id\", \"labels\") # If you used 'label_id'\n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "    print(\"\nTokenized Datasets:\")\n",
    "    print(tokenized_datasets)\n",
    "else:\n",
    "    print(\"Skipping tokenization as no data was loaded.\")\n",
    "    tokenized_datasets = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Selection\n",
    "\n",
    "Choose a pre-trained model from Hugging Face Hub that is suitable for your task and compatible with Vertex AI deployment. <mcreference link=\"https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-hugging-face-models\" index=\"1\">1</mcreference>\n",
    "- **Narrative Detection / Sentiment Analysis (Text)**: `bert-base-multilingual-cased`, `xlm-roberta-base`, `distilbert-base-multilingual-cased`. For Nigerian languages, multilingual models are crucial.\n",
    "- **Image Classification (Memes/Stickers)**: `google/vit-base-patch16-224`, `facebook/convnext-tiny-224`. Check Vertex AI Model Garden for supported vision models.\n",
    "- **Audio Processing**: `facebook/wav2vec2-base-960h`, `openai/whisper-base`. \n",
    "- **Video Processing**: Often involves combining audio and image models, or specialized models like `MCG-NJU/videomae-base`. This can be more complex.\n",
    "\n",
    "The `BASE_MODEL_ID` was set in the Configuration section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is selected via BASE_MODEL_ID in the configuration section\n",
    "print(f\"Selected base model for fine-tuning: {BASE_MODEL_ID}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}